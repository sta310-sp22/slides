---
title: "Poisson Regression"
subtitle: "Goodness-of-fit & overdispersion"
author: "Prof. Maria Tackett"
date: "01.31.22"
output:
  xaringan::moon_reader:
    #mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta310-slides.css"
    logo: sta310-sticker.png
    lib_dir: libs/font-awesome
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%" 
      ratio: "16:9"
    self_contained: true
---

```{r setup, include = F}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      fig.width = 8,
                      fig.asp = 0.618, 
                      fig.retina = 3, 
                      dpt = 300, 
                      out.width = "70%",
                      fig.align = "center")

ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))

colors <- tibble::tibble(green = "#B5BA72")
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(GGally)
library(xaringanExtra)
library(knitr)
library(patchwork)
library(viridis)
library(ggfortify)
library(gridExtra)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

class: middle, center

##[Click for PDF of slides](07-poisson-pt2.pdf)

---

## Announcements

- Reading: [BMLR - Chapter 4 Poisson regression](https://bookdown.org/roback/bookdown-BeyondMLR/ch-beyondmost.html)

- [Mini-Project 01](https://sta310-sp22.github.io/assignments/projects/mini-project-01.html): 
  - Draft **due Wed, Feb 02 at 12pm (noon)** in GitHub repo
  - Peer review in class Wednesday 
  - Final write up and presentations **Wed, Feb 09**

- Thursday's class: Offsets and Zero-inflated Poisson model (ZIP)

- HW 02 **due Mon, Feb 07 at 11:59pm**
  - Released later today (will announce on GitHub Discussions)

---

## Learning goals 

- Define and calculate residuals for the Poisson regression model 

- Use Goodness-of-fit to assess model fit 

- Identify overdispersion 

- Apply modeling approaches to deal with overdispersion 

---

class: middle, inverse

## Recap 

---

## The data: Household size in the Philippines

The data [fHH1.csv](data/fHH1.csv) come from the 2015 Family Income and Expenditure Survey conducted by the Philippine Statistics Authority. 

**Goal**: Understand the association between household size and various characteristics of the household

**Response**: 
- `total`: Number of people in the household other than the head

.left[
**Predictors**: 
- `location`: Where the house is located
- `age`: Age of the head of household
- `roof`: Type of roof on the residence (proxy for wealth)
]

.right[
**Other variables**: 
- `numLT5`: Number in the household under 5 years old 
]

```{r echo = F}
hh_data <- read_csv("data/fHH1.csv")
```

---

## Poisson regression model 

If $Y_i \sim Poisson$ with $\lambda = \lambda_i$ for the given values $x_{i1}, \ldots, x_{ip}$, then 

.eq[
$$\log(\lambda_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}$$
]


--

- Each observation can have a different value of $\lambda$ based on its value of the predictors $x_1, \ldots, x_p$

- $\lambda$ determines the mean and variance, so we don't need to estimate a separate error term 

---

## Model 1: Household vs. Age

```{r}
model1 <- glm(total ~ age, data = hh_data, family = poisson)

tidy(model1) %>% 
  kable(digits = 4)
```

$$\log(\hat{\lambda}) = 1.5499  - 0.0047 ~ age$$

The mean household size is predicted to decrease by 0.47% for each year older the head of the household is.

---

## Model 2: Add a quadratic effect for `age`

```{r}
hh_data <- hh_data %>% 
  mutate(age2 = age*age)

model2 <- glm(total ~ age + age2, data = hh_data, family = poisson)
tidy(model2, conf.int = T) %>% 
  kable(digits = 4)
```

---

## Add `location` to the model? 

```{r}
model3 <- glm(total ~ age + age2 + location, data = hh_data, family = poisson)
```

--

Use a **drop-in-deviance** test to determine if Model 2 or Model 3 (with location) is a better fit for the data.

--


```{r}
anova(model2, model3, test = "Chisq") %>%
  kable(digits = 3)
```

The p-value is small (0.01 < 0.05), so we conclude that Mode 3 is a better fit for the data. 

---

## Model 3

```{r echo = F}
tidy(model3, conf.int = TRUE) %>%
  kable(digits = 4)
```

--

.vocab[Does this model sufficiently explain the variability in the mean household size?]

---

class: middle, inverse

## Goodness-of-fit

---

## Pearson residuals

We can calculate two types of residuals for Poisson regression: Pearson residuals and deviance residuals  

.eq[
$$\text{Pearson residual}_i = \frac{\text{observed} - \text{predicted}}{\text{std. error}} = \frac{y_i - \hat{\lambda}_i}{\sqrt{\hat{\lambda}_i}}$$
]

- Similar interpretation as residuals from linear regression 

- Expect most to fall between -2 and 2

- Used to calculate overdispersion parameter

---

## Deviance residuals 

The **deviance residual** indicates how much the observed data deviates from the fitted model

.eq[
$$\text{deviance residual}_i = \text{sign}(y_i - \hat{\lambda}_i)\sqrt{2\Bigg[y_i\log\bigg(\frac{y_i}{\hat{\lambda}_i}\bigg) - (y_i - \hat{\lambda}_i)\Bigg]}$$

where 

$$\text{sign}(y_i - \hat{\lambda}_i)  =  \begin{cases}
1 & \text{ if }(y_i - \hat{\lambda}_i) > 0 \\
-1 & \text{ if }(y_i - \hat{\lambda}_i) < 0 \\
0 & \text{ if }(y_i - \hat{\lambda}_i) = 0
\end{cases}$$
]

Recall $(\text{residual deviance}) = \sum_{i=1}^{n}(\text{deviance residual})_i^2$

---

## Model 3: Residual plots 


```{r}
model3_aug_pearson <- augment(model3, type.residuals = "pearson") 
model3_aug_deviance <- augment(model3, type.residuals = "deviance")
```


```{r echo = F}
p1 <- ggplot(data = model3_aug_pearson, aes(x = .fitted, y = .resid)) + 
  geom_point()  + 
  geom_smooth() + 
  labs(x = "Fitted values", 
       y = "Pearson residuals", 
       title = "Pearson residuals vs. fitted")

p2 <-  ggplot(data = model3_aug_deviance, aes(x = .fitted, y = .resid)) + 
  geom_point()  + 
  geom_smooth() + 
  labs(x = "Fitted values", 
       y = "Deviance residuals", 
       title = "Deviance residuals vs. fitted")

p1 + p2
```

---

## Goodness-of-fit

- **Goal**: Use the (residual) deviance to assess how much the predicted values differ from the observed values 

- If the model sufficiently fits the data, then 

$$\text{deviance} \sim \chi^2_{df}$$ 

  where $df$ is the model's residual degrees of freedom 

- **Question to answer**: What is the probability of observing a deviance larger than the one we've observed, given this model sufficiently fits the data?

$$P(\chi^2_{df} > \text{ deviance})$$

---

## Model 3: Goodness-of-fit

```{r}
model3$deviance
model3$df.residual
```

--

```{r}
pchisq(model3$deviance, model3$df.residual, lower.tail = FALSE)
```

The probability of observing a deviance greater than `r round(model3$deviance,1)` is $\approx 0$, so there is significant evidence of **lack-of-fit**. 

---

## Lack-of-fit

There are a few potential reasons for the lack-of-fit

- Missing important interactions or higher-order terms

- Missing important variables (perhaps this means a more comprehensive data set is required)

- There could be extreme observations causing the deviance to be larger than expected (not observed in our data)

- There could be a problem with the Poisson model 
  - May need more flexibility in the model to handle **overdispersion**
  
---

## Overdispersion 

**Overdispersion**: There is more variability in the response than what is implied by the Poisson model 

.pull-left[
.center[.vocab[Overall]]

```{r echo = F}
hh_data %>%
  summarise(mean = mean(total), var = var(total)) %>%
  kable(digits = 3)
```
]

--

.pull-right[
.center[.vocab[by Location]]

```{r echo = F}
hh_data %>%
  group_by(location) %>%
  summarise(mean = mean(total), var = var(total)) %>%
  kable(digits = 3)
```
]

---

## Why overdispersion matters

If there is overdispersion, then there is more variation in the response than what's implied by a Poisson model. This means 

`r emo::ji("x")` The standard errors of the model coefficients are artificially small 

`r emo::ji("x")` The p-values are artificially small 

`r emo::ji("x")` This could lead to models that are more complex than what is needed 

--

We can take overdispersion into account by
  - inflating standard errors by multiplying them by a dispersion factor
  - using a negative-binomial regression model.

---

class: middle, inverse

## Quasi-poission 

---

## Dispersion parameter

The **dispersion parameter** is represented by $\phi$

.eq[
$$\hat{\phi} =\frac{\text{deviance}}{\text{residual df}} = \frac{\sum_{i=1}^{n}(\text{Pearson residuals})^2}{n - p}$$

where $p$ is the number of terms in the model (including the intercept)
]

- If there is no overdispersion $\hat{\phi} = 1$

- If there is overdispersion $\hat{\phi} >  1$

---

## Accounting for dispersion in the model 

- We inflate the standard errors of the coefficient by multiplying the variance by $\hat{\phi}$

$$SE_{Q}(\hat{\beta}) = \sqrt{\hat{\phi}}  * SE(\hat{\beta})$$
- "Q" stands for **quasi-Poisson**, since this is an ad-hoc solution 
  - The process for model building and model comparison is called **quasilikelihood**


---

## Model 3: Quasi-Poisson model 

```{r}
model3_q <- glm(total ~ age + age2 + location, data = hh_data, 
                family = quasipoisson) #<<
```

```{r, echo = F}
tidy(model3_q, conf.int = T) %>% kable(digits = 4)
```
---

## Poisson vs. Quasi-Poisson models 

.pull-left[

.center[.vocab[**Poisson**]]

```{r echo = F}
tidy(model3) %>%
  select(term, estimate, std.error) %>%
  kable(digits = 4)
```
]


.pull-right[

.center[.vocab[**Quasi-Poisson**]]

```{r echo = F}
tidy(model3_q) %>%
  select(estimate, std.error) %>%
  kable(digits = 4)
```
]

---

## Quasi-Poisson: Inference for coefficients

.pull-left[
```{r echo = F}
tidy(model3_q) %>%
  select(term, estimate, std.error) %>%
  kable(digits = 4)
```
]

.pull-right[
.center[**Test statistic**]
$$t = \frac{\hat{\beta} - 0}{SE_{Q}(\hat{\beta})} \sim t_{n-p}$$
]


---

class: middle, inverse

## Negative binomial regression model 

---

## Negative binomial regression model 

Another approach to handle overdispersion is to use a **negative binomial regression model** 

- This has more flexibility than the quasi-Poisson model, because there is a new parameter in addition to $\lambda$

<br> 

--

Let $Y$ be a **negative binomial random variable**, $Y\sim NegBinom(r, p)$, then 

$$P(Y = y_i) = {y_i + r - 1 \choose r - 1}(1-p)^{y_i}p^r \hspace{5mm} y_i = 0, 1, 2, \ldots, \infty$$

---

## Negative binomial regression model 

- **Main idea**: Generate a $\lambda$ for each observation (household) and generate a count using the Poisson random variable with parameter $\lambda$ 
  - Makes the counts more dispersed than with a single parameter 
  
- Think of it as a Poisson model such that $\lambda$ is also random 
--

.eq[
$$\begin{aligned} &\text{If }Y|\lambda \sim Poisson(\lambda)\\
&\text{ and } \lambda \sim Gamma\bigg(r, \frac{1-p}{p}\bigg)\\
&\text{ then } Y \sim NegBinom(r, p)\end{aligned}$$
]

---

## Negative binomial simulation exercise

---

## Negative binomial regression in R


```{r}
library(MASS)
model3_nb <- glm.nb(total ~ age + age2 + location, data = hh_data)
tidy(model3_nb) %>% 
  kable(digits = 4)
```


---


## Acknowledgements

These slides are based on content in [BMLR - Chapter 4 Poisson regression ](https://bookdown.org/roback/bookdown-BeyondMLR/ch-beyondmost.html)


