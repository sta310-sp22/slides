---
title: "Poisson Regression"
author: "Prof. Maria Tackett"
date: "01.26.22"
output:
   xaringan::moon_reader:
    #mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta310-slides.css"
    logo: sta310-sticker.png
    lib_dir: libs/font-awesome
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%" 
      ratio: "16:9"
    self_contained: true
---

```{r setup, include = F}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      fig.width = 8,
                      fig.asp = 0.618, 
                      fig.retina = 3, 
                      dpt = 300, 
                      out.width = "70%",
                      fig.align = "center")

ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))

colors <- tibble::tibble(green = "#B5BA72")
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(GGally)
library(xaringanExtra)
library(knitr)
library(patchwork)
library(viridis)
library(ggfortify)
library(gridExtra)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

class: middle, center

##[Click for PDF of slides](06-poisson-pt1.pdf)

---

## Announcements

- Week 03 & 04 reading: 
  - [BMLR: Chapter 4 - Poisson Regression](https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html)

- Quiz 01 due Thu, Jan 27 at 3:30pm (start of lab)

- [Mini-project 01: Analysis plan](https://sta310-sp22.github.io/assignments/projects/mini-project-01.html#Analysis_plan) due Thu, Jan 27 at 11:59pm for feedback (optional)

---

## Learning goals 

- Describe properties of the Poisson random variable

- Write the Poisson regression model 

- Describe how the Poisson regression differs from least-squares regression

- Interpret the coefficients for the Poisson regression model 

- Compare two Poisson regression models

---

## Scenarios to use Poisson regression 

- Does the number of employers conducting on-campus interviews during a year differ for public and private colleges?

- Does the daily number of asthma-related visits to an Emergency Room differ depending on air pollution indices?

- Does the number of paint defects per square foot of wall differ based on the years of experience of the painter? 

---

## Scenarios to use Poisson regression 

- Does the .vocab[number of employers conducting on-campus interviews during a year] differ for public and private colleges?

- Does the .vocab[daily number of asthma-related visits to an Emergency Room] differ depending on air pollution indices?

- Does the .vocab[number of paint defects per square foot of wall] differ based on the years of experience of the painter? 

<br> 

--


Each response variable is a .vocab[count per a unit of time or space].

---

## Poisson distribution

Let $Y$ be the number of events in a given unit of time or space. Then $Y$ can be modeled using a .vocab[Poisson distribution]

$$P(Y=y) = \frac{e^{-\lambda}\lambda^y}{y!} \hspace{10mm} y=0,1,2,\ldots, \infty$$

--

.vocab[Features]

- $E(Y) = Var(Y) = \lambda$ 
- The distribution is typically skewed right, particularly if $\lambda$ is small
- The distribution becomes more symmetric as $\lambda$ increases
  - If $\lambda$ is sufficiently large, it can be approximated using a normal distribution ([Click here](https://online.stat.psu.edu/stat414/lesson/28/28.2) for an example.)
  
---

```{r echo = F, out.width = "60%"}
set.seed(2000)
sim1 <- rpois(100000,1)
sim2 <- rpois(100000,5)
sim3 <- rpois(100000,50)
pois_sim <- tibble (
  sim1 = sim1, 
  sim2 = sim2, 
  sim3 = sim3
)
p1 <- ggplot(data = pois_sim, aes(x = sim1)) +
  geom_histogram() +
  labs(x = "", title = "lambda = 1")
p2 <- ggplot(data = pois_sim, aes(x = sim2)) +
  geom_histogram() +
  labs(x = "", title = "lambda = 5")
p3 <- ggplot(data = pois_sim, aes(x = sim3)) +
  geom_histogram() +
  labs(x = "", title = "lambda = 50")
p1 + p2 + p3 
```


```{r echo = F}
sum1 <- c(mean(sim1), var(sim1))
sum2 <- c(mean(sim2), var(sim2))
sum3 <- c(mean(sim3), var(sim3))
data <- rbind(sum1,sum2,sum3)
rownames(data) <- c("lambda = 1", "lambda = 5","lambda = 50")
colnames(data) <- c("Mean", "Variance")
kable(data,format="html")
```
  
---

## Example 

The annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean 6.5. **What is the probability there will be at 3 or fewer such earthquakes next year?**

--

$$P(Y \leq 3) = P(Y = 0) + P(Y = 1) + P(Y = 2) + P(Y = 3)$$

--

$$ = \frac{e^{-6.5}6.5^0}{0!} + \frac{e^{-6.5}6.5^1}{1!} + \frac{e^{-6.5}6.5^2}{2!} + \frac{e^{-6.5}6.5^3}{3!}$$
--

$$ = 0.112$$

--

```{r}
ppois(3, 6.5)
```

<br>

.footnote[Example adapted from [Introduction to Probability Theory Example 28-2](https://online.stat.psu.edu/stat414/lesson/28/28.2)]

---

class: middle, inverse

## Poisson regression

---

## The data: Household size in the Philippines

The data [fHH1.csv](data/fHH1.csv) come from the 2015 Family Income and Expenditure Survey conducted by the Philippine Statistics Authority. 

**Goal**: Understand the association between household size and various characteristics of the household

**Response**: 
- `total`: Number of people in the household other than the head

.left[
**Predictors**: 
- `location`: Where the house is located
- `age`: Age of the head of household
- `roof`: Type of roof on the residence (proxy for wealth)
]

.right[
**Other variables**: 
- `numLT5`: Number in the household under 5 years old 
]

---

## The data 

```{r}
hh_data <- read_csv("data/fHH1.csv")
hh_data %>% slice(1:5) %>% kable()
```

---

## Response variable

.pull-left[
```{r, echo = F, out.width = "100%"}
ggplot(data = hh_data, aes(x = total)) +
  geom_histogram() + 
  labs(title = "Total number in household other than the head")
```
]

.pull-left[
```{r echo = F}
hh_data %>%
  summarise(mean = mean(total), var = var(total)) %>%
  kable(digits = 3)
```
]
---

## Why the least-squares model doesn't work

The goal is to model $\lambda$, the expected number of people in the household (other than the head), as a function of the predictors (covariates)

--

We might be tempted to try a linear model $$\lambda_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip}$$

--

This model won't work because...

- It could produce negative values of $\lambda$ for certain values of the predictors 
- The equal variance assumption required to conduct inference for linear regression is violated. 

---

## Poisson regression model 

If $Y_i \sim Poisson$ with $\lambda = \lambda_i$ for the given values $x_{i1}, \ldots, x_{ip}$, then 

.eq[
$$\log(\lambda_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}$$
]

--

- Each observation can have a different value of $\lambda$ based on its value of the predictors $x_1, \ldots, x_p$

- $\lambda$ determines the mean and variance, so we don't need to estimate a separate error term 

---

## Poisson vs. multiple linear regression 

```{r, OLSpois, fig.align="center",out.width="60%", fig.cap='Regression models: Linear regression (left) and Poisson regression (right).',echo=FALSE, warning=FALSE, message=FALSE}
## Sample data for graph of OLS normality assumption
## Code from https://stackoverflow.com/questions/31794876/ggplot2-how-to-curve-small-gaussian-densities-on-a-regression-line?rq=1

set.seed(0)
dat <- data.frame(x=(x=runif(10000, 0, 50)),
                  y=rnorm(10000, 10*x, 100))

## breaks: where you want to compute densities
breaks <- seq(0, max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)

## Get the residuals
dat$res <- residuals(lm(y ~ x, data=dat))

## Compute densities for each section, flip the axes, add means 
## of sections.  Note: densities need to be scaled in relation 
## to section size (2000 here)
dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
  d <- density(x$res, n=5000)
  res <- data.frame(x=max(x$x)- d$y*1000, y=d$x+mean(x$y))
  res <- res[order(res$y), ]
  ## Get some data for normal lines as well
  xs <- seq(min(x$res), max(x$res), len=5000)
  res <- rbind(res, data.frame(y=xs + mean(x$y),
                x=max(x$x) - 1000*dnorm(xs, 0, sd(x$res))))
  res$type <- rep(c("empirical", "normal"), each=5000)
  res
}))
dens$section <- rep(levels(dat$section), each=10000)

ols_assume <- ggplot(dat, aes(x, y)) +
  geom_point(size = 0.1, alpha = .25) +
  geom_smooth(method="lm", fill=NA, lwd=2) +
  geom_path(data=dens[dens$type=="normal",], 
            aes(x, y, group=section), 
            color="salmon", lwd=1.1) +
  theme_bw() +
  geom_vline(xintercept=breaks, lty=2)

# Now make Poisson regression picture
set.seed(0)
dat <- data.frame(x=(x=runif(1000, 0, 20)),
                  y=rpois(1000, exp(.1*x)))

## breaks: where you want to compute densities
breaks <- seq(2, max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)

## Get the residuals
dat$res <- dat$y - .1*dat$x

## Compute densities for each section, flip the axes, add means
## of sections.  Note: densities need to be scaled in relation 
## to section size
dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
  d <- density(x$res, n=500)
  res <- data.frame(x=max(x$x)- d$y*10, y=d$x+mean(x$y))
  res <- res[order(res$y), ]
  ## Get some data for poisson lines as well
  xs <- seq(min(x$y), max(x$y), len=500)
  res <- rbind(res, data.frame(y=xs,
          x=max(x$x) - 10*dpois(round(xs), exp(.1*max(x$x)))))
  res$type <- rep(c("empirical", "poisson"), each=500)
  res
}))
dens$section <- rep(levels(dat$section), each=1000)

pois_assume <- ggplot(dat, aes(x, jitter(y, .25))) +
  geom_point(size = 0.1) +
  geom_smooth(method="loess", fill=NA, lwd=2) +
  geom_path(data=dens[dens$type=="poisson",], 
            aes(x, y, group=section), 
            color="salmon", lwd=1.1) +
  theme_bw() + ylab("y") + xlab("x") +
  geom_vline(xintercept=breaks, lty=2)

grid.arrange(ols_assume, pois_assume, ncol = 2)
```

.footnote[From [BMLR Figure 4.1](https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html#a-graphical-look-at-poisson-regression)]

---

## Assumptions for Poisson regression 

.pull-left[
**Poisson response**: The response variable is a count per unit of time or space, described by a Poisson distribution, at each level of the predictor(s)

**Independence**: The observations must be independent of one another

**Mean = Variance**: The mean must equal the variance

**Linearity**: The log of the mean rate, $\log(\lambda)$, must be a linear function of the predictor(s)
]

.pull-right[
```{r echo = F, out.width = "100%"}
pois_assume
```

]

---

class: middle, inverse

## Model 1: Number in household vs. age


---

## Model 1: Household vs. Age

```{r}
model1 <- glm(total ~ age, data = hh_data, family = poisson)

tidy(model1) %>% 
  kable(digits = 4)
```

$$\log(\hat{\lambda}) = 1.5499  - 0.0047 ~ age$$

---

.question[
The coefficient for `age` is -0.0047. Interpret this coefficient in context. Select all that apply. [Click here](https://forms.gle/L6oTrTQzgh27FzVK9) to submit your response.

.small[a. The mean household size is predicted to decrease by 0.0047 for each year older the head of the household is.]

.small[b. The mean household size is predicted to multiply by a factor of `r round(exp(-0.0047),4)` for each year older the head of the household is.]

.small[c. The mean household size is predicted to decrease by `r round(exp(-0.0047),4)` for each year older the head of the household is.]

.small[d. The mean household size is predicted to multiply by a factor of 0.47% for each year older the head of the household is.] 

.small[e. The mean household size is predicted to decrease by 0.47% for each year older the head of the household is.]

]
]

---

.question[
The coefficient for `age` is -0.0047. Interpret this coefficient in context. Select all that apply. [Click here](https://forms.gle/L6oTrTQzgh27FzVK9) to submit your response.

.small[a. The mean household size is predicted to decrease by 0.0047 for each year older the head of the household is.]

.small[b. The mean household size is predicted to multiply by a factor of `r round(exp(-0.0047),4)` for each year older the head of the household is.]

.small[c. The mean household size is predicted to decrease by `r round(exp(-0.0047),4)` for each year older the head of the household is.]

.small[d. The mean household size is predicted to multiply by a factor of 0.47% for each year older the head of the household is.] 

.small[e. The mean household size is predicted to decrease by 0.47% for each year older the head of the household is.]

]
]

```{r echo = F}
library(countdown)
countdown(minutes = 3, seconds = 0,
          margin = "5%")
```

---

## Understanding the interpretation 

Let's derive the change in predicted mean when we go from $x$ to $x+1$ 

(see boardwork)


---

## Is the coefficient of `age` statistically significant? 

```{r echo = F}
tidy(model1, conf.int = T) %>%
  kable(digits = 4)
```

$$H_0: \beta_1 = 0 \hspace{2mm} \text{ vs. } \hspace{2mm} H_a: \beta_1 \neq 0$$

--

**Test statistic**

$$Z = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)} = \frac{-0.0047 - 0}{0.0009} = -5.026 \text{ (using exact values)}$$

--

**P-value**

$$P(|Z| > |-5.026|) = 5.01 \times 10^{-7} \approx 0$$ 

---

## What are plausible values for the coefficient of `age`? 

```{r echo = F}
tidy(model1, conf.int = T) %>%
  kable(digits = 4)
```

**95% confidence interval for the coefficient of `age`**

$$\hat{\beta}_1 \pm Z^{*}\times SE(\hat{\beta}_1)$$
$$-0.0047 \pm 1.96 \times 0.0009 = \mathbf{(-.0065, -0.0029)}$$

.question[
Interpret the interval in terms of the change in mean household size. 
]

---

**Which plot can best help us determine whether Model 1 is a good fit?**

```{r echo = F}
p1 <- ggplot(data = hh_data, aes(x = age, y = total)) + 
  geom_point() + 
  labs(y = "Total household size", 
       title = "Plot A")

p2 <- hh_data %>%
  group_by(age) %>% 
  summarise(mean = mean(total)) %>%
  ggplot(aes(x = age, y = mean))+ 
  geom_point() + 
  labs(y = "Empirical mean household size", 
       title = "Plot B")

p3 <- hh_data %>%
  group_by(age) %>% 
  summarise(log_mean = log(mean(total))) %>%
  ggplot(aes(x = age, y = log_mean)) + 
  geom_point() + 
  labs(y = "Log empirical mean household size", 
       title = "Plot C")

p1 + p2 + p3 + plot_annotation(tag_levels = 'A')
```

---

class: middle, inverse

## Model 2: Add a quadratic effect for `age`

---

## Model 2: Add a quadratic effect for `age`

```{r}
hh_data <- hh_data %>% 
  mutate(age2 = age*age)

model2 <- glm(total ~ age + age2, data = hh_data, family = poisson)
tidy(model2, conf.int = T) %>% 
  kable(digits = 4)
```

---

## Model 2: Add a quadratic effect for `age`

```{r echo = F}
tidy(model2, conf.int = T) %>% 
  kable(digits = 4)
```

We can determine whether to keep $age^2$ in the model in two ways: 

`r emo::ji("one")` Use the p-value (or confidence interval) for the coefficient (since we are adding a single term to the model)

`r emo::ji("two")` Conduct a drop-in-deviance test

---

## Deviance 

A **deviance** is a way to measure how the observed data deviates from the model predictions.

- It's a measure unexplained variability in the response variable (similar to SSE in linear regression ) 

- Lower deviance means the model is a better fit to the data 

--

We can calculate the "deviance residual" for each observation in the data (more on the formula later). Let $(\text{deviance residual}_i$ be the deviance residual for the $i^{th}$ observation, then 

$$\text{deviance} = \sum(\text{deviance residual})_i^2$$

*Note: Deviance is also known as the "residual deviance"*

---

## Drop-in-Deviance Test

We can use a **drop-in-deviance test** to compare two models. To conduct the test 

`r emo::ji("one")` Compute the deviance for each model 

`r emo::ji("two")` Calculate the drop in deviance 

$$\text{drop-in-deviance =  Deviance(reduced model) - Deviance(larger model)}$$

`r emo::ji("three")` Given the reduced model is the true model $(H_0 \text{ true})$, then $$\text{drop-in-deviance} \sim \chi_d^2$$

where $d$ is the difference in degrees of freedom between the two models (i.e., the difference in number of terms)

---

## Drop-in-deviance to compare Model1 and Model2

```{r}
anova(model1, model2, test = "Chisq") %>%
  kable(digits = 3)
```

.question[
- Write the null and alternative hypotheses.
- What does the value 2337.089	tell you? 
- What does the value 1 tell you? 
- What is your conclusion? 
]

---

## Add `location` to the model? 

Suppose we want to add `location` to the model, so we compare the following models: 

**Model A**: $\lambda_i = \beta_0 + \beta_1 ~ age_i + \beta_2 ~ age_i^2$

**Model B**: $\lambda_i =  \beta_0 + \beta_1 ~ age_i + \beta_2 ~ age_i^2 + \beta_3 ~ Loc1_i + \beta_4 ~ Loc2_i + \beta_5 ~ Loc3_i + \beta_6 ~ Loc4_i$

.question[
.small[Which of the following are reliable ways to determine if `location` should be added to the model? Select all that apply. [Click here](https://forms.gle/G3oDWo8ifhTPqCf67) to submit your response.]

Drop-in-deviance test,  Use the p-value for each coefficient, Likelihood ratio test, Nested F Test, BIC
]


---


## Looking ahead 

- For next time - [Chapter 4 - Poisson Regression](https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html)
  - Sections 4.6, 4.10


---


## Acknowledgements

These slides are based on content in [BMLR Chapter 4 - Poisson regression](https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html)
   - Focused on sections 4.4 - 4.5, 4.9


