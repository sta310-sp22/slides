---
title: "Review of multiple linear regression"
sutitle: "Cont'd"
author: "Prof. Maria Tackett"
date: "01.12.22"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta310-slides.css"
    logo: sta310-sticker.png
    lib_dir: libs/font-awesome
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%" 
      ratio: "16:9"
---

```{r setup, include = F}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      fig.width = 8,
                      fig.asp = 0.618, 
                      fig.retina = 3, 
                      dpt = 300, 
                      out.width = "90%",
                      fig.align = "center")

ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))

colors <- tibble::tibble(green = "#B5BA72")
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(GGally)
library(xaringanExtra)
library(knitr)
library(patchwork)
library(viridis)
library(ggfortify)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

class: middle, center

##[Click for PDF of slides](03-mlr-review-pt2.pdf)

---

## Announcements

- Lab starts Thu 3:30 - 4:45pm online
  - Find Zoom link in Sakai

- Office hours this week: 
  - Thu 2 - 3pm & Fri 1 - 2pm online (links in Sakai)
  - Full office hours schedule starts Tue, Jan 19

- <font color = "red">Week 02 reading</font>

---

class: middle

## Questions?

---

class: middle, inverse

## Recap

---

## Data: Kentucky Derby Winners

.midi[
Today's data is from the Kentucky Derby, an annual 1.25-mile horse race held at the Churchill Downs race track in Louisville, KY. The data is in the file [derbyplus.csv](data/derbyplus.csv) and contains information for races 1896 - 2017. 
]

.pull-left[

.midi[**Response variable**]

- .midi[`speed`: Average speed of the winner in feet per second (ft/s)]

.midi[**Additional variable**]
.midi[- `winner`: Winning horse]
]

.pull-right[
.midi[**Predictor variables**]
- .midi[`year`: Year of the race]
- .midi[`condition`: Condition of the track (good, fast, slow)]
- .midi[`starters`: Number of horses who raced]
]

---

## Data

```{r}
derby <- read_csv("data/derbyplus.csv") %>%
  mutate(yearnew = year - 1896)
```

```{r}
derby %>%
  head(5) %>% kable()
```

---

## Model 1: Main effects model (with centering)

```{r echo = F}
model1Cent <- lm(speed ~ starters + yearnew + condition, data = derby)
tidy(model1Cent) %>% kable(digits = 3)
```

---

## Model 2: Include quadratic effect for year

.midi[
```{r echo = F}
model2 <- lm(speed ~ starters + yearnew + I(yearnew^2) + condition, 
             data = derby)
tidy(model2) %>% kable(digits = 4)
```
]

---

## Model 2: Check model assumptions

```{r, echo = F, out.width = "70%"}
autoplot(model2)
```

---

class: middle, inverse

## Model 3
---

## Include interaction term?

Recall from the EDA...

```{r echo = F, out.width = "70%"}
library(viridis)
ggplot(data = derby, aes(x = year, y = speed, color = condition, 
                         shape = condition, linetype = condition)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE, aes(linetype = condition)) + 
  labs(x = "Year", y = "Speed (ft/s)", color = "Condition",
       title = "Speed vs. year", 
       subtitle = "by track condition") +
  guides(lty = FALSE, shape = FALSE) +
  scale_color_viridis_d(end = 0.9)
```

---

## Model 3: Include interaction term

$$\begin{aligned}\widehat{speed} = & 52.387 - 0.003 ~ starters + 0.020 ~ yearnew - 1.070 ~ good - 2.183 ~ slow \\ &+0.012 ~ yearnew \times good + 0.012 ~ yearnew \times slow \end{aligned}$$

.panelset.sideways[
.panel[.panel-name[Output]
```{r echo = F ,out.width = "70%"}
model3 <- lm(speed ~ starters + yearnew + condition +
               yearnew * condition, 
             data = derby)
tidy(model3) %>% kable(digits = 3)
```
]

.panel[.panel-name[Code]
```{r model3-code, eval = F}
model3 <- lm(speed ~ starters + yearnew + condition +
               yearnew * condition, 
             data = derby)
tidy(model3) %>% kable(digits = 4)
```
]
.panel[.panel-name[Assumptions]
```{r, echo = F}
autoplot(model3)
```
]
]

---

## Interpreting interaction effects

```{r echo = F}
tidy(model3) %>%
  kable(digits = 3)
```


[Click here](https://forms.gle/BrufiFdhQAi4WBWeA) for poll


```{r echo = F}
library(countdown)
countdown(minutes = 4, seconds = 0,
          margin = "5%")
```

---

## Measures of model performance

- $\color{#4187aa}{R^2}$ Proportion of variability in the response explained by the model.]
  - .midi[Will always increase as predictors are added, so it shouldn't be used to compare models]
  
- $\color{#4187aa}{Adj. R^2}$: Similar to $R^2$ with a penalty for extra terms

--

- $\color{#4187aa}{AIC}$: Likelihood-based approach balancing model performance and complexity

- $\color{#4187aa}{BIC}$: Similar to AIC with stronger penalty for extra terms

--

- **Nested F Test (extra sum of squares F test)**: Generalization of t-test for individual coefficients to perform significance tests on nested models

---

## Which model would you choose?

Use the **`glance`** function to get model statistics.

```{r echo = F}
model1_glance <- glance(model1Cent) %>%
  select(r.squared, adj.r.squared, AIC, BIC)
model2_glance <- glance(model2) %>%
  select(r.squared, adj.r.squared, AIC, BIC)
model3_glance <- glance(model3) %>%
  select(r.squared, adj.r.squared, AIC, BIC)

model1_glance %>%
  bind_rows(model2_glance) %>%
  bind_rows(model3_glance) %>%
  bind_cols(model = c("Model1", "Model2", "Model3")) %>%
  select(model, everything()) %>%
kable(digits = 3)
```

**Which model would you choose?**
---

## Characteristics of a "good" final model

- Model can be used to answer primary research questions
- Predictor variables control for important covariates
- Potential interactions have been investigated
- Variables are centered, as needed, for more meaningful interpretations 
- Unnecessary terms are removed 
- Assumptions are met and influential points have been addressed
- Model tells a "persuasive story parsimoniously"

<br>

.footnote[List from Section 1.6.7 of [BMLR](https://bookdown.org/roback/bookdown-BeyondMLR/)]

---

class: middle, inverse

## Inference for multiple linear regression

---

## Inference for regression 

Use statistical inference to 

- Determine if predictors are statistically significant (not necessarily practically significant!)
- Quantify uncertainty in coefficient estimates
- Quantify uncertainty in model predictions

<br>

If L.I.N.E. assumptions are met, we can conduct inference using the $t$ distribution and estimated standard errors 

---

## Recall the picture and the estimated standard error 


---

## Hypohtesis testing 

---

## Confidence intervals 

---

## Prediction intervals


---

# Muddiest point? 




---

## Acknowledgements

These slides are based on content in [BMLR: Chapter 1 - Review of Multiple Linear Regression](https://bookdown.org/roback/bookdown-BeyondMLR/ch-MLRreview.html)




